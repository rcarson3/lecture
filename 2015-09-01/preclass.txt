## Reading questions

The first two questions are questions from last time, but worth
revisiting.  These are up rather late, but do what you can, and come
with questions for class!

1.  The class cluster consists of eight nodes and fifteen Xeon Phi
    accelerator boards.  Based on an online search for information on
    these systems, what do you think is the theoretical peak flop rate
    (double-precision floating point operations per second)?  Show how
    you computed this, and give URLs for where you got the parameters
    in your calculation.  (We will return to this question again after
    we cover some computer architecture.)
    
    Based on the similar info given in your lecture:
    Xeon CPU
    Max flop/s = 2 Flops/FMA * 4 FMA/vector FMA * 2 vector FMA/cycle * 12 cores * 3.2 Gcycle/s * 8 nodes = 4915.2 GFlop/s
    Xeon Phi
    Max flop/s = 2 Flops/FMA * 4 FMA/vector FMA * 2 vector FMA/cycle * 60 cores * 1.053 Gcycle/s * 15 nodes = 15163.2 GFlop/s
    
    Total
    
    Xeon Phi + Xeon CPU
    
    Max Flop/s = 20 TFlop/s
    
    http://ark.intel.com/products/71992/Intel-Xeon-Phi-Coprocessor-5110P-8GB-1_053-GHz-60-core
    http://ark.intel.com/products/83352/Intel-Xeon-Processor-E5-2620-v3-15M-Cache-2_40-GHz
    https://www-ssl.intel.com/content/www/us/en/high-performance-computing/high-performance-xeon-phi-coprocessor-brief.html page 3 shows roughly 1011 GFlop/s per board which comes out roughly to what we calculated.
    
    Assume that the Xeon Phi has similar FMA/cycle as a haswell architecture most likely a bad assumption though
    
    
2.  What is the approximate theoretical peak flop rate for your own machine?

Used info from your lecture on parallel architecture since mine has the same exact architecture i5-4228U

Max flops = 2 Flops/FMA * 4 FMA/vector FMA * 2 vector FMA/cycle * 2 corers * 2.6 Gcycle/s = 83.2 GFlop/s without boost
Max flops = 2 Flops/FMA * 4 FMA/vector FMA * 2 vector FMA/cycle * 2 corers * 3.1 Gcycle/s = 99.1 GFlop/s with boost

Would need to take into account also the amount of latency per cycle for this processor since this should lead to a lower amount of theoretical flops than predicted here. It might also be possible that the threads might change the amount of flops as well.


3.  Suppose there are t tasks that can be executed in a pipeline
    with p stages.  What is the speedup over serial execution of the
    same tasks?
    
    If one assumes that none of the tasks depend upon each other than the following speed up can be assummed.
    
    In serial, if the tasks each take x amount of time to run than the total amount of time to run is t*x. If we have p stages than we know that the minimum time taken is x if p>=t. So then we would see a speed up of t. Else, the speed up would be around t/(ceil(t/p)).

4.  Consider the following list of tasks (assume they can't be pipelined):

      compile GCC (1 hr)
      compile OpenMPI (0.5 hr) - depends on GCC
      compile OpenBLAS (0.25 hr) - depends on GCC
      compile LAPACK (0.5 hr) - depends on GCC and OpenBLAS
      compile application (0.5 hr) - depends on GCC, OpenMPI,
        OpenBLAS, LAPACK

    What is the minimum serial time between starting to compile and having
    a compiled application?  What is the minimum parallel time given
    an arbitrary number of processors?
    
    The minimum seriel time would be: 1hr+0.5hr*3+0.25hr = 2.75 hr
    The minimum parallel time would be: 1 hr + 0.75 hr + 0.5 hr = 2.25 hr.

5.  Clone the membench repository from GitHub:

       git clone git@github.com:cornell-cs5220-f15/membench.git

    On your own machine, build `membench` and generate the associated
    plots; for many of you, this should be as simple as typing `make`
    at the terminal (though I assume you have Python with pandas and
    Matplotlib installed; see also the note about Clang and OpenMP
    in the leading comments of the Makefile).  Look at the output file
    timings-heat.pdf; what can you tell about the cache architecture
    on your machine from the plot?
    
    It's apparrent from the plot that there are quite a few cache misses around a stride length of 64 KBs when the data is larger than then the L1 cache size of 64 KBs. It suggests that the memory is having to then access the addresses from either different CPU caches L2 and L3 or from virtual memory which leads to even slower times.

6.  From the cloned repository, check out the totient branch:

       git checkout totient

    You may need to move generated files out of the way to do this.
    If you prefer, you can also look at the files on GitHub.  Either
    way, repeat the exercise of problem 5.  What can you tell about
    the cache architecture of the totient nodes?
    
    Based on the heat map, it appears to have a rather large L2 cache. Since, it doesn't appear to be taking to severe of a hit up until the memory starts reaching a couple MBs. So, it is able to keep a lot of the data in the cache where it has quick memory access and only when it starts having to read outside this cache size does one start to see major hits in read times. Several of the missed hits could also be due to the fact that as the stride length starts getting larger it can not evenly fit into the L2 cache of the CPUs, so the CPU has to fetch the additional range of data in the virtual memory of the machine.

7.  Implement the following three methods of computing the centroid
    of a million two-dimensional coordinates (double precision).
    Time and determine which is faster:

    a.  Store an array of (x,y) coordinates; loop i and simultaneously
        sum the xi and yi

    b.  Store an array of (x,y) coordinates; loop i and sum the xi,
        then sum the yi in a separate loop

    c.  Store the xi in one array, the yi in a second array.
        Sum the xi, then sum the yi.

    I recommend doing this on the class cluster using the Intel
    compiler.  To do this, run "module load cs5220" and run (e.g.)

        icc -o centroid centroid.c
        
First, off we know C is row memory access, so any operations that don't take advantage of this will be slower. 
The results when run off my laptop are: 

Version 1: 3.956920e-03
Version 2: 7.242100e-03
Version 3: 7.359920e-03

